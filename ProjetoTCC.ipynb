{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Definição do problema\n",
    "> O objetivo deste trabalho é apresentar uma análise sobre os preços de produtos importados pelo Brasil, comparando os valores declarados numa operação de importação com os preços praticados no comércio internacional.\n",
    ">Para isso, primeiramente, será realizado um levantamento de preços junto ao mercado mundial para um grupo de mercadorias de alto valor agregado.\n",
    "> Em seguida, buscar-se-ão os preços desses mesmos produtos declarados em processos de importação de mercadorias.\n",
    "> Por fim, far-se-á a comparação entre as médias desses valores, visando a uma conclusão quanto a indícios de fraudes - subfaturamento ou superfaturamento - nas operações de importação realizadas pelo importador nacional.\n",
    "\n",
    "### 2. A coleta de dados\n",
    "> Os datasets usados neste trabalho foram coletados de fontes diversas. \n",
    "> Definiu-se que os sites __www.aliexpress.com__ e __www.alibaba.com__, notórios fornecedores de produtos diversos, serão objeto de webscrapping realizado com a ferramenta de código aberto _Beautifull Soup_ e _Selenium_, as quais serão utilizadas para coletar uma gama de aproximadamente 1 mil registros de cada um desses sites.\n",
    "> Para delimitar o escopo da análise, será definido apenas uma gama de produtos, que apresenta uma variação grande de preços: __display lcd para aparelhos celulares__.\n",
    "\n",
    "__Bold por favor__ <br>\n",
    "_Italico Olha isso_ <br>\n",
    "$ a + bX$ <br>\n",
    "__________ dfjlkaj dlkjf lkajkdj\n",
    "> item \n",
    ">>item2\n",
    ">>>item3\n",
    "- item 1\n",
    "- item 2\n",
    "1. item 1\n",
    "2. item 2 <br>\n",
    "<font color=blue>\"font color=blue\"</font><br>\n",
    "paragrafog <br> outro paragrafo<br>\n",
    "#isto é comentário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saiu#############\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4f0528ca90b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"organic-list app-organic-search__list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m#Agora vamos buscar os itens da lista\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'organic-gallery-offer-outter J-offer-wrapper'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ul'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"list-items\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "#bibliotecas para requisição de páginas dinâmicas\n",
    "from selenium import webdriver \n",
    "#from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "    \n",
    "def alteraIdioma(driver):\n",
    "    #Este passo é necessário para mudar o foco do mouse para o driver\n",
    "    user_id = driver.find_element_by_xpath('//*[@id=\"nav-user-account\"]') \n",
    "    ActionChains(driver).move_to_element(user_id).click().perform()\n",
    "    time.sleep(3)\n",
    "\n",
    "    #Aqui começo a navegar para alterar o idioma\n",
    "    driver.find_element_by_xpath('//*[@id=\"switcher-info\"]').click()\n",
    "    time.sleep(3)\n",
    "    painel = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div/div/div[4]/div/div')\n",
    "    painel.click()\n",
    "    #painel country selector switcher\n",
    "    time.sleep(3)\n",
    "    driver.find_element_by_xpath('/html/body/div[1]/div[3]/div/div/div[4]/div/div/div/div[1]/div/a[1]/span/span').click()\n",
    "    time.sleep(3)\n",
    "    painel.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[4]/div/div/div/div[1]/div/div[1]/ul/li[224]\").click()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath('/html/body/div[1]/div[3]/div/div/div[4]/div/div/div/div[4]/button').click()\n",
    "    driver.refresh()           \n",
    "    return driver\n",
    "\n",
    "\n",
    "\n",
    "#Preparando a aquisição das páginas\n",
    "    #Criando informação sobre quem está fazendo o scrap\n",
    "headers = {\n",
    "    'User-Agent': 'João Lima, from PUC-MG University',\n",
    "    'From': 'jbetol@gmail.com'\n",
    "}\n",
    "\n",
    "#Ainda precisa Verificar com Try Except se driver está dispoível\n",
    "try: #if os.path.exists('/media/joao/HDLinux/tools/'):\n",
    "    driver = webdriver.Firefox('/media/joao/HDLinux/tools')\n",
    "except IOError:\n",
    "    sys.exit(\"Geckodriver not found!\")\n",
    "    \n",
    "url_aliexpress = 'https://www.aliexpress.com/wholesale?trafficChannel=main&d=y&CatId=0&SearchText=phone+display+lcd+for+screen&ltype=wholesale&SortType=default&page='\n",
    "\n",
    "url_alibaba = 'https://www.alibaba.com/products/phone_display_lcd_screen/CID5904002.html?spm=a2700.galleryofferlist.0.0.126c1524tZVgH8&IndexArea=product_en&page='\n",
    "\n",
    "#Vamos pegar 50 páginas de dados e jogar num array\n",
    "paginas = []\n",
    "qnt_paginas = 2\n",
    "passou = False\n",
    "alibaba = False  # Vamos começar scrapping por AliExpress.com\n",
    "site_index = 2  # números de sites a ser scrapping: AliExpress e Alibaba\n",
    "descricao = []\n",
    "preco_min = []\n",
    "preco_max = [] #_items = {} # Dicionário que vai guardar os dados das páginas in memory \n",
    "while site_index > 0  :\n",
    "    site_index = site_index - 1\n",
    "    for i in range(1,qnt_paginas):\n",
    "        if not alibaba:\n",
    "            url = url_aliexpress + str(i)\n",
    "        else:\n",
    "            url = url_alibaba + str(i)\n",
    "\n",
    "        paginas.append(url)\n",
    "\n",
    "    #loop para navegar em 30 páginas de aquisição de dados\n",
    "    for pagina in paginas:\n",
    "        driver.get(url_aliexpress+'100')\n",
    "\n",
    "        if not alibaba and not passou:\n",
    "            alteraIdioma(driver)\n",
    "            passou = True\n",
    "\n",
    "    #Verificar modo silencioso para o geckodriver\n",
    "        \n",
    "        req = driver.page_source\n",
    "\n",
    "\n",
    "        soup = BeautifulSoup(req, \"html.parser\")\n",
    "\n",
    "        #Aqui é para páginas estáticas. Não usa o webdriver\n",
    "        #req = requests.get(url)\n",
    "        #soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        #Adquirindo a lista dos produtos que se encontra na classe indicada\n",
    "        if alibaba:\n",
    "            classes = soup.find('div', class_ = \"organic-list app-organic-search__list\")\n",
    "            #Agora vamos buscar os itens da lista\n",
    "            items =  classes.find_all(class_ = 'organic-gallery-offer-outter J-offer-wrapper')\n",
    "        else:\n",
    "            classes = soup.find('ul', class_ = \"list-items\")\n",
    "            #Agora vamos buscar os itens da lista\n",
    "            if not classes:\n",
    "                print(\"Saiu#############\")\n",
    "                break\n",
    "            else:\n",
    "                items =  classes.find_all('li', class_ = 'list-item')\n",
    "\n",
    "        #Para cada item, pegar apenas descrição e o preco e guardar num dicionário\n",
    "        for i,item in enumerate(items):\n",
    "            if alibaba:\n",
    "                descricao.append(item.find('h4').get('title'))\n",
    "                preco = item.find('p', class_ = 'elements-offer-price-normal').get('title')\n",
    "            else:\n",
    "                descricao.append(item.find('a', class_ = 'item-title').get('title'))\n",
    "                preco = item.find('span', class_ = 'price-current').text\n",
    "\n",
    "            preco_min_max = re.findall(r'(\\d+[\\.,]\\d{1,2})', preco)\n",
    "            if len(preco_min_max) == 1:\n",
    "                preco_min_max.insert(0,'0.00') \n",
    "            preco_min.append(preco_min_max[0])\n",
    "            preco_max.append(preco_min_max[1])\n",
    "            \n",
    "    alibaba = True\n",
    "    paginas.clear()\n",
    "\n",
    "driver.quit()\n",
    "#transformando o dicionário em DataFrame do Pandas\n",
    "df = pd.DataFrame({\n",
    "    'descricao': descricao,\n",
    "    'preco_minimo': preco_min,\n",
    "    'preco_maximo': preco_max\n",
    "    })\n",
    "    #\"desc_items, columns=['Item','Descrição','Preço Mínimo', 'Preço Máximo'])\n",
    "df.info()\n",
    "df.head()\n",
    "# displaying the DataFrame  \n",
    "print('DataFrame:\\n', df) \n",
    "\n",
    "# saving the DataFrame as a CSV file \n",
    "gfg_csv_data = df.to_csv('lista.csv', header = True) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('$10.63-$16.82', '10.63', '16.82')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "dici = {}\n",
    "fonte = '$10.63-$16.82' # - US $43.77 - 44.27 $15.00-$ 39.81 US $18.49 - US325,30 US $12,70'\n",
    "fonte_limpa = re.findall(r'(\\d+[\\.,]\\d{1,2})', fonte) #re.sub(r'[aA-Zz$\\-\\s+]','',fonte)\n",
    "#fonte_limpa = re.sub(r'[aA-Zz]','',fonte_limpa)\n",
    "dici = fonte,fonte_limpa[0], fonte_limpa[1]\n",
    "print(dici)\n",
    "\n",
    "#print(re.findall(r'(\\d+[\\.,]\\d{1,2})', fonte))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}