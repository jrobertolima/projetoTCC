{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Definição do problema\n",
    "> O objetivo deste trabalho é apresentar uma análise sobre os preços de produtos importados pelo Brasil, comparando os valores declarados numa operação de importação com os preços praticados no comércio internacional.\n",
    ">Para isso, primeiramente, será realizado um levantamento de preços junto ao mercado mundial para um grupo de mercadorias de alto valor agregado.\n",
    "> Em seguida, buscar-se-ão os preços desses mesmos produtos declarados em processos de importação de mercadorias.\n",
    "> Por fim, far-se-á a comparação entre as médias desses valores, visando a uma conclusão quanto a indícios de fraudes - subfaturamento ou superfaturamento - nas operações de importação realizadas pelo importador nacional.\n",
    "\n",
    "### 2. A coleta de dados\n",
    "> Os datasets usados neste trabalho foram coletados de fontes diversas. \n",
    "> Definiu-se que os sites __www.aliexpress.com__ e __www.alibaba.com__, notórios fornecedores de produtos diversos, serão objeto de webscrapping realizado com a ferramenta de código aberto _Beautifull Soup_ e _Selenium_, as quais serão utilizadas para coletar uma gama de aproximadamente 1 mil registros de cada um desses sites.\n",
    "> Para delimitar o escopo da análise, será definido apenas uma gama de produtos, que apresenta uma variação grande de preços: __display lcd para aparelhos celulares__.\n",
    "\n",
    "__Bold por favor__ <br>\n",
    "_Italico Olha isso_ <br>\n",
    "$ a + bX$ <br>\n",
    "__________ dfjlkaj dlkjf lkajkdj\n",
    "> item \n",
    ">>item2\n",
    ">>>item3\n",
    "- item 1\n",
    "- item 2\n",
    "1. item 1\n",
    "2. item 2 <br>\n",
    "<font color=blue>\"font color=blue\"</font><br>\n",
    "paragrafog <br> outro paragrafo<br>\n",
    "#isto é comentário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 32 entries, 0 to 31\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   descricao     32 non-null     object\n 1   preco_minimo  32 non-null     object\n 2   preco_maximo  32 non-null     object\n 3   alibaba       32 non-null     bool  \ndtypes: bool(1), object(3)\nmemory usage: 928.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "#bibliotecas para requisição de páginas dinâmicas\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "    \n",
    "def alteraIdioma():\n",
    "    #Mudando o foco do mouse para a paǵina inicial\n",
    "    user_id = driver.find_element_by_xpath('//*[@id=\"nav-user-account\"]') \n",
    "    ActionChains(driver).move_to_element(user_id).click().perform()\n",
    "    time.sleep(3)\n",
    "\n",
    "    #Aqui começo a navegar para alterar o idioma\n",
    "    driver.find_element_by_xpath('//*[@id=\"switcher-info\"]').click()\n",
    "    time.sleep(3)\n",
    "    painel = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div/div/div[4]/div/div')\n",
    "    painel.click()\n",
    "    #painel country selector switcher\n",
    "    time.sleep(3)\n",
    "    driver.find_element_by_xpath('/html/body/div[1]/div[3]/div/div/div[4]/div/div/div/div[1]/div/a[1]/span/span').click()\n",
    "    time.sleep(3)\n",
    "    painel.find_element_by_xpath(\"/html/body/div[1]/div[3]/div/div/div[4]/div/div/div/div[1]/div/div[1]/ul/li[224]\").click()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath('/html/body/div[1]/div[3]/div/div/div[4]/div/div/div/div[4]/button').click()\n",
    "    driver.refresh()           \n",
    "\n",
    "#Preparando a aquisição das páginas\n",
    "#Criando informação sobre quem está fazendo o scrapping\n",
    "headers = {\n",
    "    'User-Agent': 'João Lima, from PUC-MG University',\n",
    "    'From': 'jbetol@gmail.com'\n",
    "}\n",
    "\n",
    "#Ainda precisa Verificar com Try Except se driver para Firefox (o browser que escolhi)está disponível\n",
    "try:\n",
    "    driver = webdriver.Firefox('/media/joao/HDLinux/tools')\n",
    "except IOError:\n",
    "    sys.exit(\"Geckodriver not found!\")\n",
    "\n",
    "#Faremos o scrapping nos sites AliExpress e AliBaba, idioma inglês e dólar como moeda.   \n",
    "url_aliexpress = 'https://www.aliexpress.com/wholesale?trafficChannel=main&d=y&CatId=0&SearchText=phone+display+lcd+for+screen&ltype=wholesale&SortType=default&page='\n",
    "\n",
    "url_alibaba = 'https://www.alibaba.com/products/phone_display_lcd_screen/CID5904002.html?spm=a2700.galleryofferlist.0.0.126c1524tZVgH8&IndexArea=product_en&page='\n",
    "\n",
    "paginas = []\n",
    "qnt_paginas = 99\n",
    "passou_aliexpress = False # Para controlar a mudança de idioma e moeda\n",
    "alibaba = False # Vamos começar scrapping por AliExpress.com\n",
    "site_index = 2  # números de sites a ser scrapping: AliExpress e Alibaba\n",
    "descricao = []  #descrição das mercadorias tela de lcd para smartphone\n",
    "preco_min = []\n",
    "preco_max = []\n",
    "baba_or_express = [] #Valor booleano para saber origem do item, se AliExpress oou Alibaba\n",
    "\n",
    "#Vamos pegar 99 páginas de dados (no máximo) de cada site escolhido e jogar num array\n",
    "while site_index > 0  :\n",
    "    site_index = site_index - 1\n",
    "\n",
    "    #Montando a lista de páginas\n",
    "    for i in range(1,qnt_paginas):\n",
    "        if not alibaba:\n",
    "            url = url_aliexpress + str(i)\n",
    "        else:\n",
    "            url = url_alibaba + str(i)\n",
    "\n",
    "        paginas.append(url)\n",
    "\n",
    "    #Navegando nas páginas de aquisição de dados\n",
    "    for pagina in paginas:\n",
    "        driver.get(pagina)\n",
    "\n",
    "        #O site AliExpress pode estar em português, então temos que alterar para inglês \n",
    "        if not alibaba and not passou:\n",
    "            alteraIdioma()\n",
    "            passou = True\n",
    "\n",
    "        req = driver.page_source\n",
    "        soup = BeautifulSoup(req, \"html.parser\")\n",
    "\n",
    "    #Adquirindo a lista dos produtos que se encontra na classe indicada\n",
    "        if alibaba:\n",
    "            classes = soup.find('div', class_ = \"organic-list app-organic-search__list\")\n",
    "            #Agora vamos buscar os itens da lista\n",
    "            items =  classes.find_all(class_ = 'organic-gallery-offer-outter J-offer-wrapper')\n",
    "        else:\n",
    "            classes = soup.find('ul', class_ = \"list-items\")\n",
    "            #Agora vamos buscar os itens da lista\n",
    "            try:\n",
    "                items =  classes.find_all('li', class_ = 'list-item')\n",
    "            except AttributeError: # É possível que tenhamos menos páginas do que previsto\n",
    "                break\n",
    "\n",
    "        #Para cada item, pegar apenas descrição e o preco e guardar num dicionário\n",
    "        for i,item in enumerate(items):\n",
    "            if alibaba:\n",
    "                descricao.append(item.find('h4').get('title'))\n",
    "                preco = item.find('p', class_ = 'elements-offer-price-normal').get('title')\n",
    "            else:\n",
    "                descricao.append(item.find('a', class_ = 'item-title').get('title'))\n",
    "                preco = item.find('span', class_ = 'price-current').text\n",
    "\n",
    "            baba_or_express.append(alibaba)\n",
    "            preco_min_max = re.findall(r'(\\d+[\\.,]\\d{1,2})', preco)\n",
    "            if len(preco_min_max) == 1:\n",
    "                preco_min_max.insert(0,'0.00') \n",
    "            preco_min.append(preco_min_max[0])\n",
    "            preco_max.append(preco_min_max[1])\n",
    "            \n",
    "    alibaba = True\n",
    "    paginas.clear()\n",
    "\n",
    "driver.quit()\n",
    "#transformando o dicionário em DataFrame do Pandas\n",
    "df = pd.DataFrame({\n",
    "    'descricao': descricao,\n",
    "    'preco_minimo': preco_min,\n",
    "    'preco_maximo': preco_max,\n",
    "    'alibaba': baba_or_express\n",
    "    })\n",
    "df.info()\n",
    "# saving the DataFrame as a CSV file \n",
    "gfg_csv_data = df.to_csv('lista.csv', header = True) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('$10.63-$16.82', '10.63', '16.82')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "dici = {}\n",
    "fonte = '$10.63-$16.82' # - US $43.77 - 44.27 $15.00-$ 39.81 US $18.49 - US325,30 US $12,70'\n",
    "fonte_limpa = re.findall(r'(\\d+[\\.,]\\d{1,2})', fonte) #re.sub(r'[aA-Zz$\\-\\s+]','',fonte)\n",
    "#fonte_limpa = re.sub(r'[aA-Zz]','',fonte_limpa)\n",
    "dici = fonte,fonte_limpa[0], fonte_limpa[1]\n",
    "print(dici)\n",
    "\n",
    "#print(re.findall(r'(\\d+[\\.,]\\d{1,2})', fonte))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}